{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6ad3cb",
   "metadata": {},
   "source": [
    "# 데이터 다운로드 하기\n",
    ": 인터넷에서 지정된 파일을 내 PC에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6738f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 되었습니다!\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import urllib.request\n",
    "\n",
    "# URL과 저장 경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "urllib.request.urlretrieve(url, savename) # (주소, 저장경로)\n",
    "print(\"저장 되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6231b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import urllib.request\n",
    "\n",
    "# URL과 저장 경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test1.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "# 파일로 저장하기\n",
    "with open(savename, \"wb\") as f: # 이미지파일 wb, 텍스트파일 w\n",
    "    f.write(mem)\n",
    "print(\"저장되었습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fed32e",
   "metadata": {},
   "source": [
    "# BeautifulSoup로 Scraping하기\n",
    ": 간단하게 HTML과 XML에서 정보를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1294de6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/tj/opt/anaconda3/lib/python3.8/site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/tj/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 # 모듈 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b55c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "<body>\n",
      "<h1>Header</h1>\n",
      "<p> Line 1을 서술 </p>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1>Header</h1>\n",
    "            <p> Line 1을 서술 </p>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6e4c893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Header</h1>\n",
      "<p> Line 1을 서술 </p>\n",
      "-----------\n",
      "h1= Header\n",
      "h1= Header\n",
      "p1=  Line 1을 서술 \n",
      "p1=  Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# 원하는 부분 추출하기\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1)\n",
    "print(p1)\n",
    "print(\"-----------\")\n",
    "\n",
    "# Text만 출력\n",
    "print(\"h1=\", h1.string)\n",
    "print(\"h1=\", h1.text)\n",
    "print(\"p1=\", p1.string)\n",
    "print(\"p1=\", p1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41ed9834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\"> Header </h1>\n",
      "<p id=\"body\"> Line 1을 서술 </p>\n",
      "title =   Header \n",
      "body =   Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# id로 요소를 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1 id=\"title\"> Header </h1>\n",
    "            <p id=\"body\"> Line 1을 서술 </p>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 원하는 부분 추출하기(find는 맨 처음 1개의 요소만 가져옴)\n",
    "title = soup.find(id = \"title\")\n",
    "body = soup.find(id = 'body')\n",
    "print(title)\n",
    "print(body)\n",
    "\n",
    "# text만 출력\n",
    "print(\"title = \", title.string)\n",
    "print(\"body = \", body.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73c35a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.naver.com\">naver</a>,\n",
       " <a href=\"http://www.daum.net\">daum</a>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 개의 요소 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <ul>\n",
    "                <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "                <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "            </ul>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 조건에 해당하는 걸 전부 가져오기(단순 text형태)\n",
    "links = soup.find_all('a')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21f635c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver >>>> http://www.naver.com\n",
      "daum >>>> http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "# 원하는 부분 전부 추출하기\n",
    "for link in links:\n",
    "    #print(link.string) # 내용 가져오기\n",
    "    #print(link.attrs['href']) # 링크 가져오기(하이퍼링크 포함)\n",
    "    \n",
    "    href = link.attrs['href'] # 링크 가져오기(하이퍼링크 포함)\n",
    "    text = link.string # 내용 가져오기\n",
    "    print(text, \">>>>\", href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "243ade21",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-69b0aa81b11d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-69b0aa81b11d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    http://www.weather.go.kr/weather/forecast/mid-term-rss.jsp?stnld=109\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "http://www.weather.go.kr/weather/forecast/mid-term-rss.jsp?stnld=109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cdcfaa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "--------------------\n",
      "<class 'list'>\n",
      "['○ (강수) 16일(금)~17일(토)에는 전국(수도권은 16일, 강원영동 제외)에 소나기가 오는 곳이 있겠습니다.', '          18일(일) 오전에 제주도에 비가 시작되어 18일(일) 오후에 충청권과 남부지방으로 확대되겠고, 19일(월) 오후에는 전국에 비가 오겠습니다.', '○ (기온) 이번 예보기간 아침 기온은 23~26도, 낮 기온은 29~34도로 어제(12일, 아침최저기온 23~26도, 낮최고기온 30~34도)와 비슷하겠습니다.', '○ (주말전망) 17일(토) 오후에 강원영서와 충청권, 남부지방, 제주도에 소나기가 오는 곳이 있겠습니다.', '              18일(일) 오전에 제주도에 비가 시작되어, 오후에는 충청권과 남부지방으로 확대되겠습니다. 아침 기온은 23~25도, 낮 기온은 29~33도가 되겠습니다.', ' * 이번 예보기간 동안 내륙을 중심으로 낮최고기온이 33도 이상, 아침최저기온이 25도 이상으로 올라 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다.', ' * 또한, 북태평양고기압 위치에 따라서 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.']\n",
      "--------------------\n",
      "○ (강수) 16일(금)~17일(토)에는 전국(수도권은 16일, 강원영동 제외)에 소나기가 오는 곳이 있겠습니다.\n",
      "18일(일) 오전에 제주도에 비가 시작되어 18일(일) 오후에 충청권과 남부지방으로 확대되겠고, 19일(월) 오후에는 전국에 비가 오겠습니다.\n",
      "○ (기온) 이번 예보기간 아침 기온은 23~26도, 낮 기온은 29~34도로 어제(12일, 아침최저기온 23~26도, 낮최고기온 30~34도)와 비슷하겠습니다.\n",
      "○ (주말전망) 17일(토) 오후에 강원영서와 충청권, 남부지방, 제주도에 소나기가 오는 곳이 있겠습니다.\n",
      "18일(일) 오전에 제주도에 비가 시작되어, 오후에는 충청권과 남부지방으로 확대되겠습니다. 아침 기온은 23~25도, 낮 기온은 29~33도가 되겠습니다.\n",
      "* 이번 예보기간 동안 내륙을 중심으로 낮최고기온이 33도 이상, 아침최저기온이 25도 이상으로 올라 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다.\n",
      "* 또한, 북태평양고기압 위치에 따라서 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "# 기상청 자료 활용하기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss.jsp?stnld=109\"\n",
    "\n",
    "# data 가져오기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# print(soup) - 확인용\n",
    "\n",
    "# 원하는 데이터 추출하기\n",
    "title = soup.find('title').string\n",
    "print(title) # 결과는 기상청 육상 중기예보\n",
    "wf = soup.find('wf').string # wf 태그에 해당하는 내용을 string 자료형으로 가져오기\n",
    "#print(wf) # 결과는 bs4.element.CData\n",
    "\n",
    "\n",
    "# '<br />'을 제외한 리스트 만들기\n",
    "listwf = wf.split('<br />')\n",
    "listwf?\n",
    "print('-'*20)\n",
    "print(type(listwf))\n",
    "print(listwf)\n",
    "print('-'*20)\n",
    "for i in listwf:\n",
    "    print(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79f9880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "○ (강수) 16일(금)~17일(토)에는 전국(수도권은 16일, 강원영동 제외)에 소나기가 오는 곳이 있겠습니다.           18일(일) 오전에 제주도에 비가 시작되어 18일(일) 오후에 충청권과 남부지방으로 확대되겠고, 19일(월) 오후에는 전국에 비가 오겠습니다. ○ (기온) 이번 예보기간 아침 기온은 23~26도, 낮 기온은 29~34도로 어제(12일, 아침최저기온 23~26도, 낮최고기온 30~34도)와 비슷하겠습니다. ○ (주말전망) 17일(토) 오후에 강원영서와 충청권, 남부지방, 제주도에 소나기가 오는 곳이 있겠습니다.               18일(일) 오전에 제주도에 비가 시작되어, 오후에는 충청권과 남부지방으로 확대되겠습니다. 아침 기온은 23~25도, 낮 기온은 29~33도가 되겠습니다.  * 이번 예보기간 동안 내륙을 중심으로 낮최고기온이 33도 이상, 아침최저기온이 25도 이상으로 올라 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다.  * 또한, 북태평양고기압 위치에 따라서 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' (강수) 16일(금)~17일(토)에는 전국(수도권은 16일, 강원영동 제외)에 소나기가 오는 곳이 있겠습니다.           18일(일) 오전에 제주도에 비가 시작되어 18일(일) 오후에 충청권과 남부지방으로 확대되겠고, 19일(월) 오후에는 전국에 비가 오겠습니다. ',\n",
       " ' (기온) 이번 예보기간 아침 기온은 23~26도, 낮 기온은 29~34도로 어제(12일, 아침최저기온 23~26도, 낮최고기온 30~34도)와 비슷하겠습니다. ',\n",
       " ' (주말전망) 17일(토) 오후에 강원영서와 충청권, 남부지방, 제주도에 소나기가 오는 곳이 있겠습니다.               18일(일) 오전에 제주도에 비가 시작되어, 오후에는 충청권과 남부지방으로 확대되겠습니다. 아침 기온은 23~25도, 낮 기온은 29~33도가 되겠습니다.  * 이번 예보기간 동안 내륙을 중심으로 낮최고기온이 33도 이상, 아침최저기온이 25도 이상으로 올라 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다.  * 또한, 북태평양고기압 위치에 따라서 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 정리하기\n",
    "list_wfs = list(wf)\n",
    "except_char = ['<','b','r','/','>']\n",
    "result = \"\"\n",
    "\n",
    "for lwf in list_wfs:\n",
    "    #print(lwf)\n",
    "    if lwf not in except_char:\n",
    "        result += lwf\n",
    "        \n",
    "print('-' * 100)\n",
    "print(result) # 자료형 str\n",
    "print(type(result))\n",
    "result.split('○') # 자료형 list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d2816",
   "metadata": {},
   "source": [
    "# css 선택자 사용하기\n",
    "\n",
    "soup.select_one() : css 선택자로 요소 하나를 추출.   \n",
    "soup.select() : css 선택자로 요소 여러 개를 리스트로 추출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61d30dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>위키 북스</h1>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <div id = 'meigen'>\n",
    "                <h1>위키 북스</h1>\n",
    "                <ul class=\"items\">\n",
    "                    <li>유니티 게임 이펙트 입문서</li>\n",
    "                    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "                    <li>모던 웹사이트 디자인의 정석</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 필요한 부분 css로 추출하기\n",
    "# 타이틀 부분 추출하기\n",
    "\n",
    "# id = 'meigen'의 자손 h1을 찾기\n",
    "h1 = soup.select_one(\"div#meigen > h1\") \n",
    "\n",
    "# ---정의---\n",
    "# id:#\n",
    "# class:.\n",
    "# > :자손\n",
    "# \"  \" : 후손\n",
    "print(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4cdfb879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li>유니티 게임 이펙트 입문서</li>, <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>, <li>모던 웹사이트 디자인의 정석</li>]\n"
     ]
    }
   ],
   "source": [
    "# 목록 부분 추출하기\n",
    "li_lists = soup.select(\"div#meigen > ul.items > li\")\n",
    "print(li_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47636640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니티 게임 이펙트 입문서\n",
      "스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "# 목록 부분 텍스트만 추출하기\n",
    "for li in li_lists:\n",
    "    print(li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 금융에서 환율 정보 추출하기\n",
    "# https://finance.naver.com/marketindex/\n",
    "\n",
    "# 개발자 도구 - Copy Selector\n",
    "# #exchangeList > li.on > a.head.usd > div > span.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e29dfb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd /krw :  1,143.50\n"
     ]
    }
   ],
   "source": [
    "# 미국 환율 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"usd /krw : \", price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "089f8ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"value\">1,144.00</span>, <span class=\"value\">1,036.42</span>, <span class=\"value\">1,358.16</span>, <span class=\"value\">176.85</span>, <span class=\"value\">110.2700</span>, <span class=\"value\">1.1867</span>, <span class=\"value\">1.3894</span>, <span class=\"value\">92.2500</span>, <span class=\"value\">74.1</span>, <span class=\"value\">1626.84</span>, <span class=\"value\">1805.5</span>, <span class=\"value\">66549.47</span>]\n"
     ]
    }
   ],
   "source": [
    "# 미국, 일본, 유럽연합, 중국의 환율\n",
    "# HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "price_list = soup.select(\"div.head_info > span.value\")\n",
    "print(price_list) # list 내역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "90e6fe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,144.00\n",
      "1,036.42\n",
      "1,358.16\n",
      "176.85\n",
      "110.2700\n",
      "1.1867\n",
      "1.3894\n",
      "92.2500\n",
      "74.1\n",
      "1626.84\n",
      "1805.5\n",
      "66549.47\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 text로 추출\n",
    "for price in price_list:\n",
    "    print(price.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "21f1d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 :  1,144.00\n",
      "일본 :  1,036.42\n",
      "유럽연합 :  1,358.16\n",
      "중국 :  176.85\n"
     ]
    }
   ],
   "source": [
    "# 정리\n",
    "print(\"미국 : \", price_list[0].string)\n",
    "print(\"일본 : \", price_list[1].string)\n",
    "print(\"유럽연합 : \", price_list[2].string)\n",
    "print(\"중국 : \", price_list[3].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ed41e71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "<class 'list'>\n",
      "하늘과 바람과 별과 시\n",
      "서시\n",
      "자화상\n",
      "소년\n",
      "눈 오는 지도\n",
      "돌아와 보는 밤\n",
      "병원\n",
      "새로운 길\n",
      "간판 없는 거리\n",
      "태초의 아침\n",
      "또 태초의 아침\n",
      "새벽이 올 때까지\n",
      "무서운 시간\n",
      "십자가\n",
      "바람이 불어\n",
      "슬픈 족속\n",
      "눈감고 간다\n",
      "또 다른 고향\n",
      "길\n",
      "별 헤는 밤\n",
      "흰 그림자\n",
      "사랑스런 추억\n",
      "흐르는 거리\n",
      "쉽게 씌어진 시\n",
      "봄\n",
      "참회록\n",
      "간(肝)\n",
      "위로\n",
      "팔복\n",
      "못자는밤\n",
      "달같이\n",
      "고추밭\n",
      "아우의 인상화\n",
      "사랑의 전당\n",
      "이적\n",
      "비오는 밤\n",
      "산골물\n",
      "유언\n",
      "창\n",
      "바다\n",
      "비로봉\n",
      "산협의 오후\n",
      "명상\n",
      "소낙비\n",
      "한난계\n",
      "풍경\n",
      "달밤\n",
      "장\n",
      "밤\n",
      "황혼이 바다가 되어\n",
      "아침\n",
      "빨래\n",
      "꿈은 깨어지고\n",
      "산림\n",
      "이런날\n",
      "산상\n",
      "양지쪽\n",
      "닭\n",
      "가슴 1\n",
      "가슴 2\n",
      "비둘기\n",
      "황혼\n",
      "남쪽 하늘\n",
      "창공\n",
      "거리에서\n",
      "삶과 죽음\n",
      "초한대\n",
      "산울림\n",
      "해바라기 얼굴\n",
      "귀뚜라미와 나와\n",
      "애기의 새벽\n",
      "햇빛·바람\n",
      "반디불\n",
      "둘 다\n",
      "거짓부리\n",
      "눈\n",
      "참새\n",
      "버선본\n",
      "편지\n",
      "봄\n",
      "무얼 먹구 사나\n",
      "굴뚝\n",
      "햇비\n",
      "빗자루\n",
      "기왓장 내외\n",
      "오줌싸개 지도\n",
      "병아리\n",
      "조개껍질\n",
      "겨울\n",
      "트루게네프의 언덕\n",
      "달을 쏘다\n",
      "별똥 떨어진 데\n",
      "화원에 꽃이 핀다\n",
      "종시\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# 윤동주 시안 작품 가져오기\n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 작품 가져오기\n",
    "# 자료형 ResultSet\n",
    "title1 = soup.select(\"#mw-content-text > div.mw-parser-output > ul li a[title]\")\n",
    "# '>'가 없으면 그 밑의 모든 요소들을 가지고 옴\n",
    "print(type(title1))\n",
    "\n",
    "# 자료형 List\n",
    "title_list1 = list(title1)\n",
    "print(type(title_list1))\n",
    "\n",
    "\n",
    "for i in title1:\n",
    "    if i.string not in '증보판':\n",
    "        print(i.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b725043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 하늘과 바람과 별과 시\n",
      "- 서시\n",
      "- 자화상\n",
      "- 소년\n",
      "- 눈 오는 지도\n",
      "- 돌아와 보는 밤\n",
      "- 병원\n",
      "- 새로운 길\n",
      "- 간판 없는 거리\n",
      "- 태초의 아침\n",
      "- 또 태초의 아침\n",
      "- 새벽이 올 때까지\n",
      "- 무서운 시간\n",
      "- 십자가\n",
      "- 바람이 불어\n",
      "- 슬픈 족속\n",
      "- 눈감고 간다\n",
      "- 또 다른 고향\n",
      "- 길\n",
      "- 별 헤는 밤\n",
      "- 흰 그림자\n",
      "- 사랑스런 추억\n",
      "- 흐르는 거리\n",
      "- 쉽게 씌어진 시\n",
      "- 봄\n",
      "- 참회록\n",
      "- 간(肝)\n",
      "- 위로\n",
      "- 팔복\n",
      "- 못자는밤\n",
      "- 달같이\n",
      "- 고추밭\n",
      "- 아우의 인상화\n",
      "- 사랑의 전당\n",
      "- 이적\n",
      "- 비오는 밤\n",
      "- 산골물\n",
      "- 유언\n",
      "- 창\n",
      "- 바다\n",
      "- 비로봉\n",
      "- 산협의 오후\n",
      "- 명상\n",
      "- 소낙비\n",
      "- 한난계\n",
      "- 풍경\n",
      "- 달밤\n",
      "- 장\n",
      "- 밤\n",
      "- 황혼이 바다가 되어\n",
      "- 아침\n",
      "- 빨래\n",
      "- 꿈은 깨어지고\n",
      "- 산림\n",
      "- 이런날\n",
      "- 산상\n",
      "- 양지쪽\n",
      "- 닭\n",
      "- 가슴 1\n",
      "- 가슴 2\n",
      "- 비둘기\n",
      "- 황혼\n",
      "- 남쪽 하늘\n",
      "- 창공\n",
      "- 거리에서\n",
      "- 삶과 죽음\n",
      "- 초한대\n",
      "- 산울림\n",
      "- 해바라기 얼굴\n",
      "- 귀뚜라미와 나와\n",
      "- 애기의 새벽\n",
      "- 햇빛·바람\n",
      "- 반디불\n",
      "- 둘 다\n",
      "- 거짓부리\n",
      "- 눈\n",
      "- 참새\n",
      "- 버선본\n",
      "- 편지\n",
      "- 봄\n",
      "- 무얼 먹구 사나\n",
      "- 굴뚝\n",
      "- 햇비\n",
      "- 빗자루\n",
      "- 기왓장 내외\n",
      "- 오줌싸개 지도\n",
      "- 병아리\n",
      "- 조개껍질\n",
      "- 겨울\n",
      "- 트루게네프의 언덕\n",
      "- 달을 쏘다\n",
      "- 별똥 떨어진 데\n",
      "- 화원에 꽃이 핀다\n",
      "- 종시\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# 윤동주 시안 작품 가져오기\n",
    "# #mw-content-text > div.mw-parser-output > ul:nth-child(6) > li > a\n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 작품 가져오기\n",
    "a_list = soup.select(\"#mw-content-text > div.mw-parser-output > ul > li a\")\n",
    "\n",
    "for a in a_list:\n",
    "    if a.string == '증보판':\n",
    "        continue # 다시 for a in a_list: 로 올라가서 실행\n",
    "    print(\"-\", a.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b48541e",
   "metadata": {},
   "source": [
    "## 다음 영화 연간 순위 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f1e49867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 남산의 부장들\n",
      "2 : 다만 악에서 구하소서\n",
      "3 : 반도\n",
      "4 : 히트맨\n",
      "5 : 테넷\n",
      "6 : 백두산\n",
      "7 : #살아있다\n",
      "8 : 강철비2: 정상회담\n",
      "9 : 담보\n",
      "10 : 닥터 두리틀\n",
      "11 : 삼진그룹 영어토익반\n",
      "12 : 정직한 후보\n",
      "13 : 도굴\n",
      "14 : 클로젯\n",
      "15 : 오케이 마담\n",
      "16 : 해치지않아\n",
      "17 : 천문: 하늘에 묻는다\n",
      "18 : 결백\n",
      "19 : 1917\n",
      "20 : 작은 아씨들\n",
      "21 : 미드웨이\n",
      "22 : 시동\n",
      "23 : 지푸라기라도 잡고 싶은 짐승들\n",
      "24 : 미스터 주: 사라진 VIP\n",
      "25 : 인비저블맨\n",
      "26 : 나쁜 녀석들: 포에버\n",
      "27 : 국제수사\n",
      "28 : 침입자\n",
      "29 : 스타워즈: 라이즈 오브 스카이워커\n",
      "30 : 스파이 지니어스 \n",
      "31 : 이웃사촌\n",
      "32 : 온워드: 단 하루의 기적\n",
      "33 : 소리도 없이\n",
      "34 : 버즈 오브 프레이(할리 퀸의 황홀한 해방)\n",
      "35 : 원더 우먼 1984\n",
      "36 : 겨울왕국 2\n",
      "37 : 오! 문희\n",
      "38 : 그린랜드\n",
      "39 : 위대한 쇼맨\n",
      "40 : 런\n",
      "41 : 뮬란\n",
      "42 : 내가 죽던 날\n",
      "43 : 기생충\n",
      "44 : 신비아파트 극장판 하늘도깨비 대 요르문간드\n",
      "45 : 프리즌 이스케이프\n",
      "46 : 검객\n",
      "47 : 조제\n",
      "48 : 사라진 시간\n",
      "49 : 밤쉘: 세상을 바꾼 폭탄선언\n",
      "50 : 알라딘\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://movie.daum.net/ranking/boxoffice/yearly\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 영화 가져오기\n",
    "# #mainContent > div > div.box_boxoffice > ol > li:nth-child(1) > div > div.thumb_cont > strong > a\n",
    "\n",
    "movie_list = soup.select(\"strong a\")\n",
    "#print(movie_list)\n",
    "\n",
    "num = 1\n",
    "\n",
    "for movie in movie_list:\n",
    "    print(num, \":\", movie.string)\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183281b",
   "metadata": {},
   "source": [
    "## 많이 본 뉴스 가져오기\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91dbd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://v.daum.net/v/20210714200610665 : [제보는 MBC] 약국 1시간 늦게 열었다 병원 원장에 무릎 꿇은 약사\n",
      "https://v.daum.net/v/20210714193602153 : 최다확진 날 반성은커녕..서울시 부시장의 '대통령 탓'\n",
      "https://v.daum.net/v/20210714171800838 : \"일본 망할 거다\" \"오지 마라\".. 도쿄 시민들 분노 [박철현의 도쿄스캔들]\n",
      "https://v.daum.net/v/20210714175305101 : \"국민의힘 지지잔데요\"..전라고 학생, 대선배 최강욱에 당돌 질문\n",
      "https://v.daum.net/v/20210714175504157 : 전국민에 준다는 재난지원금..나랏빚 내서 부자들에게 퍼줄 판\n",
      "https://v.daum.net/v/20210714174210737 : 청년 月 10만원씩 3년 360만원 저금하고 1440만원 받는다\n",
      "https://v.daum.net/v/20210714195004389 : \"곧 2000명대\" 우려 현실화?..수도권 새 감염원 속출\n",
      "https://v.daum.net/v/20210714193640164 : 야산서 숨진 고교생 학교폭력 정황..가해 학생 입건\n",
      "https://v.daum.net/v/20210714181318691 : '1615명 확진' 정부 예측보다 빠르다..의료계는 \"1만명 각오해야\"\n",
      "https://v.daum.net/v/20210714194521313 : [사건팀장] \"내가 노는 게 아니잖아요\"..직업 두고 다투다 어머니 살해\n",
      "https://v.daum.net/v/20210714195515459 : 50대 후반 예약 오늘 밤 재개..50대 초반은 연령별 분산\n",
      "https://v.daum.net/v/20210714164607753 : 모기에 물리지 않는 옷 개발..굶주린 200마리 100% 차단\n",
      "https://v.daum.net/v/20210714173520487 : 40대 이하 화이자 접종..'광클대란' 백신도 예약 5부제\n",
      "https://v.daum.net/v/20210714193341121 : 야산서 숨진 고교생..학교폭력 정황 드러나\n",
      "https://v.daum.net/v/20210714201222738 : [뉴스추적] 이재명 캠프, 말실수로 끙끙..여배우 논란 해명\n",
      "https://v.daum.net/v/20210714183751205 : 한동훈, 박범계에 \"목적만 있고 '팩트' 없다..이런 황당한\"\n",
      "https://v.daum.net/v/20210714185154431 : [단독]윤석열, 中 향해 \"사드 문제 삼으려면 레이더 철수 먼저\"\n",
      "https://v.daum.net/v/20210714154603489 : 앞 vs 뒤.. 당신의 화장실 두루마리 휴지 방향은?\n",
      "https://v.daum.net/v/20210714202515909 : '여권 공작설' 힘 실은 윤석열..치고 빠진 이준석\n",
      "https://v.daum.net/v/20210714202724927 : 매일 커피 1잔, 코로나에 효과? 영국서 3만명 조사했더니..\n",
      "https://v.daum.net/v/20210714174437799 : \"더워 죽겠다. 돈 더 안주면 배달 안해\" 손님이 낼 배달비 오르나\n",
      "https://v.daum.net/v/20210714204056108 : 문 대통령 \"짧고 굵게\"라더니 .. \"수도권 4단계 8월까지 가야\"\n",
      "https://v.daum.net/v/20210714160010040 : \"윤석열 부부, 삼성전자서 전세금 뇌물성 지원받아\"..사세행, 공수처 고발\n",
      "https://v.daum.net/v/20210714141601641 : [영상] 1900m 절벽서 그네 타다가 줄이 뚝..러 여성 추락\n",
      "https://v.daum.net/v/20210714202845939 : [날씨] 푹푹 찌는 무더위..내일 더 강한 소나기\n",
      "https://v.daum.net/v/20210714175639223 : 흑인 아이 옆에서 'OK 손모양'.. 美놀이공원 거액 소송 당했다\n",
      "https://v.daum.net/v/20210714172637143 : 이동훈 폭로에 윤석열 움직였다..'피의사실공표' 공세 집중\n",
      "https://v.daum.net/v/20210714160038082 : 이재명 \"2번 사과\"에..'발끈' 김부선 \"가짜 사과였다\"\n",
      "https://v.daum.net/v/20210714174706911 : 獨, 밤새 시간당 667mm 폭우로 중부 전역 홍수\n",
      "https://v.daum.net/v/20210714184210292 : 오후 6시까지 신규 확진 1263명..4차 대유행 파고 올라갔다\n",
      "https://v.daum.net/v/20210714182500958 : 베트남 호찌민 코로나 확산에 한국기업들 대거 '조업 차질'(종합2보)\n",
      "https://v.daum.net/v/20210714183903234 : \"이자 부담 낮아져요\"..내일부터 금리상한 주담대 나온다\n",
      "https://v.daum.net/v/20210714184219297 : \"공짜로 잘 쓰셨죠? 이제 돈 내요\"..구글 화상회의 '수금 본색'\n",
      "https://v.daum.net/v/20210714173515480 : \"애는 왜 낳는 거죠?\"..완전히 달라진 사회 분위기 [김보미의 뉴스카페]\n",
      "https://v.daum.net/v/20210714184235300 : 알고보니 이명 아닌 뇌종양..'돌발성 난청' 자가진단법\n",
      "https://v.daum.net/v/20210714181455721 : '이젠 밤잠 설치지 마세요' 백신 사전예약 일정 변경\n",
      "https://v.daum.net/v/20210714173502470 : '대통령'은 시작일 뿐..韓공장도 태운 남아공 폭동, 왜 커지나\n",
      "https://v.daum.net/v/20210714171500726 : '20대 여성 지지율 1%' 국민의힘, 처참한 성적의 원인\n",
      "https://v.daum.net/v/20210714180216417 : 이재명 \"나는 친문이다\".. 부인 김혜경씨, '친문적자' 김경수 장인 조문하러 목포行\n",
      "https://v.daum.net/v/20210714182345931 : 심정지로 쓰러졌던 40대 임신부 만삭 출산 성공 ..국내 첫 사례\n",
      "https://v.daum.net/v/20210714203606035 : [인터뷰] 윤석열 \"장모 관련 사건, 총장 청문회 때 처음 알아\"\n",
      "https://v.daum.net/v/20210714170523398 : \"쿠팡 없이 살기 어렵다\"..코로나19 확산에 쿠팡 하루 이용자 900만명 회복\n",
      "https://v.daum.net/v/20210714181750795 : 한국 주식 17조 던진 외국인, 채권 38조 사재기..\"금리 매력\"\n",
      "https://v.daum.net/v/20210714165942179 : 임신 중에 심정지 왔지만 만삭 출산 성공..'국내최초'\n",
      "https://v.daum.net/v/20210714161354627 : 커피의 또다른 효능? \"항산·항염 성분이 코로나 감염 줄여\"\n",
      "https://v.daum.net/v/20210714194351283 : 밥 한끼 4,000원..알뜰식당 인기!\n",
      "https://v.daum.net/v/20210714160326220 : \"우리도 모르는 투명 떡볶이 열풍?\" K-떡볶이가 달라졌다[식탐]\n",
      "https://v.daum.net/v/20210714190857710 : 주식재산 100억 넘는 월급쟁이 임원 18명..1천억 주식갑부는 4명\n",
      "https://v.daum.net/v/20210714101604995 : [여기는 중국] \"믿을 수 없다!\" 韓 최저임금 발표 직후 中 포털 검색 245만 건\n",
      "https://v.daum.net/v/20210714180754564 : 月 10만원 저축땐 30만원 지원..'일자리'를 돈으로 때우는 정부\n"
     ]
    }
   ],
   "source": [
    "# 다음 IT News 많이 본 뉴스  \"링크 : 제목\" 리스트 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "#https://news.daum.net/ranking/popular\n",
    "url = \"https://news.daum.net/ranking/popular\"\n",
    "# data 가져오기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "title = soup.select('#mArticle > div.rank_news > ul.list_news2  div.cont_thumb > strong > a')\n",
    "titleText = []\n",
    "j=0\n",
    "for i in title:\n",
    "    titleText.append(i.get_text())\n",
    "    print(i['href'],\":\", titleText[j])\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05304f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d726f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05866a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb9117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b05e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
